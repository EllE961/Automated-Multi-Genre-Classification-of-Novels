{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b14c4-8fd7-4416-879a-5a7bbae4a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Package Installation -----------------------\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"\n",
    "    Installs the required packages if they are not already installed.\n",
    "    This function ensures that all dependencies are met before executing the data collection.\n",
    "    \"\"\"\n",
    "    required_packages = [\n",
    "        'requests',\n",
    "        'fuzzywuzzy',\n",
    "        'python-Levenshtein',\n",
    "        'nltk',\n",
    "        'rich',\n",
    "        'tqdm',\n",
    "        'langdetect',       # For language detection\n",
    "        'beautifulsoup4'    # For HTML parsing if needed\n",
    "    ]\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            if package == 'fuzzywuzzy':\n",
    "                __import__('fuzzywuzzy.fuzz')\n",
    "            elif package == 'beautifulsoup4':\n",
    "                __import__('bs4')\n",
    "            else:\n",
    "                __import__(package)\n",
    "        except ImportError:\n",
    "            print(f\"Installing package: {package}\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install necessary packages\n",
    "install_packages()\n",
    "\n",
    "# ----------------------- Imports -----------------------\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import logging\n",
    "import psutil\n",
    "from typing import List, Dict\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from rich.logging import RichHandler\n",
    "from rich.console import Console\n",
    "from rich.traceback import install as install_rich_traceback\n",
    "from langdetect import detect, LangDetectException\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Initialize Rich Traceback for better error messages\n",
    "install_rich_traceback()\n",
    "\n",
    "# ----------------------- Logging Configuration -----------------------\n",
    "console = Console()\n",
    "logger = logging.getLogger('DataCollectorLogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "rich_handler = RichHandler(console=console, rich_tracebacks=True)\n",
    "rich_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "rich_handler.setFormatter(formatter)\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(rich_handler)\n",
    "\n",
    "# ----------------------- Utility Functions -----------------------\n",
    "def log_memory_usage(stage: str):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024 ** 2)\n",
    "    logger.info(f'Memory Usage after {stage}: {mem:.2f} MB')\n",
    "\n",
    "def handle_exception(e: Exception, stage: str):\n",
    "    logger.error(f'Exception in {stage}: {e}', exc_info=True)\n",
    "\n",
    "# ----------------------- NLTK Resource Setup -----------------------\n",
    "def setup_nltk():\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        logger.info(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download('stopwords')\n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        logger.info(\"Downloading NLTK WordNet...\")\n",
    "        nltk.download('wordnet')\n",
    "    try:\n",
    "        nltk.data.find('corpora/omw-1.4')\n",
    "    except LookupError:\n",
    "        logger.info(\"Downloading NLTK omw-1.4...\")\n",
    "        nltk.download('omw-1.4')\n",
    "\n",
    "setup_nltk()\n",
    "\n",
    "# ----------------------- Genre Keywords Definition -----------------------\n",
    "genre_keywords = {\n",
    "    'Fiction': [\n",
    "        'fiction', 'novel', 'narrative', 'story', 'narrative prose',\n",
    "        'imaginative', 'literary', 'prose', 'literature', 'literary fiction',\n",
    "        'realistic fiction', 'contemporary fiction', 'historical fiction'\n",
    "    ],\n",
    "    'Romance': [\n",
    "        'romance', 'love stories', 'courtship', 'relationship', 'erotic', 'romantic fiction',\n",
    "        'romantic novel', 'romantic drama', 'love affair', 'passion', 'heartwarming',\n",
    "        'love triangle', 'erotica'\n",
    "    ],\n",
    "    'Mystery': [\n",
    "        'mystery', 'detective', 'crime', 'whodunit', 'noir', 'suspense', 'police procedural',\n",
    "        'forensic', 'investigation', 'thriller', 'espionage', 'conspiracy', 'cold case',\n",
    "        'locked room mystery', 'legal thriller'\n",
    "    ],\n",
    "    'Science Fiction': [\n",
    "        'science fiction', 'sci-fi', 'space', 'dystopian', 'cyberpunk', 'time travel', 'aliens',\n",
    "        'futuristic', 'post-apocalyptic', 'robotics', 'space opera', 'military sci-fi',\n",
    "        'biopunk', 'steampunk', 'hard science fiction'\n",
    "    ],\n",
    "    'Fantasy': [\n",
    "        'fantasy', 'magic', 'dragons', 'sword and sorcery', 'high fantasy', 'epic fantasy',\n",
    "        'urban fantasy', 'mythical', 'enchanted', 'supernatural', 'dark fantasy',\n",
    "        'fairy tale', 'paranormal', 'magical realism', 'heroic fantasy'\n",
    "    ],\n",
    "    'Horror': [\n",
    "        'horror', 'ghosts', 'monsters', 'supernatural', 'gothic', 'psychological horror',\n",
    "        'slasher', 'terror', 'occult', 'dark horror', 'paranormal horror', 'creepy',\n",
    "        'suspenseful', 'spooky', 'demonic', 'haunted'\n",
    "    ],\n",
    "    'Drama': [\n",
    "        'drama', 'society', 'conflict', 'melodrama', 'tragic', 'play', 'theater',\n",
    "        'emotional', 'intense relationships', 'character study', 'family drama',\n",
    "        'coming-of-age', 'social issues', 'psychological drama'\n",
    "    ],\n",
    "    'Comedy': [\n",
    "        'comedy', 'humor', 'satire', 'parody', 'slapstick', 'farce', 'irony',\n",
    "        'witty', 'lighthearted', 'humorous fiction', 'black comedy', 'romantic comedy',\n",
    "        'situational comedy', 'comedic drama'\n",
    "    ],\n",
    "    'Nonfiction': [\n",
    "        'nonfiction', 'essay', 'memoir', 'informative', 'educational', 'documentary',\n",
    "        'factual', 'realistic', 'instructional', 'geography', 'political science', \n",
    "        'economics', 'history', 'biography', 'autobiography', 'self-help', 'philosophy',\n",
    "        'science methodology', 'social science', 'psychology', 'travelogue',\n",
    "        'true crime', 'journalism', 'essay collection'\n",
    "    ],\n",
    "    'Adventure': [\n",
    "        'adventure', 'journey', 'exploration', 'quest', 'expedition', 'voyage', 'trek',\n",
    "        'travelogue', 'action-packed', 'escapade', 'survival', 'explorer', 'pioneering',\n",
    "        'daring', 'bravery', 'odyssey', 'sailing'\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ----------------------- Data Cleaning Functions -----------------------\n",
    "def extract_text_from_html(html_content: str) -> str:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        for script_or_style in soup(['script', 'style']):\n",
    "            script_or_style.decompose()\n",
    "        text = soup.get_text(separator=' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        handle_exception(e, 'Extracting Text from HTML')\n",
    "        return \"\"\n",
    "\n",
    "def clean_text_line_based(raw_text: str) -> str:\n",
    "    try:\n",
    "        raw_text = raw_text.encode('utf-8').decode('utf-8-sig')\n",
    "        lines = raw_text.splitlines()\n",
    "\n",
    "        start_idx = 0\n",
    "        end_idx = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if re.match(r'\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK', line, re.IGNORECASE):\n",
    "                start_idx = i + 1\n",
    "                break\n",
    "        else:\n",
    "            logger.warning(\"Start marker not found. Using entire text.\")\n",
    "            start_idx = 0\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if re.match(r'\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK', line, re.IGNORECASE):\n",
    "                end_idx = i\n",
    "                break\n",
    "        else:\n",
    "            logger.warning(\"End marker not found. Using entire text up to the end.\")\n",
    "            end_idx = len(lines)\n",
    "\n",
    "        content_lines = lines[start_idx:end_idx]\n",
    "\n",
    "        content_start_idx = 0\n",
    "        for i, line in enumerate(content_lines):\n",
    "            if re.match(r'^(LETTER\\s+[IVX]+\\.?|LETTER\\s+\\d+\\.?|CHAPTER\\s+\\d+\\.?|CHAPTER\\s+[IVX]+\\.?)$', line.strip(), re.IGNORECASE):\n",
    "                content_start_idx = i + 1\n",
    "                break\n",
    "        else:\n",
    "            content_start_idx = 0\n",
    "\n",
    "        content_lines = content_lines[content_start_idx:]\n",
    "\n",
    "        cleaned_content = []\n",
    "        for line in content_lines:\n",
    "            if re.match(r'^(LETTER\\s+[IVX]+\\.?|LETTER\\s+\\d+\\.?|CHAPTER\\s+\\d+\\.?|CHAPTER\\s+[IVX]+\\.?)$', line.strip(), re.IGNORECASE):\n",
    "                continue\n",
    "            cleaned_content.append(line)\n",
    "\n",
    "        cleaned_text = '\\n'.join(cleaned_content)\n",
    "        cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', cleaned_text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "\n",
    "        if not cleaned_text:\n",
    "            logger.warning(\"Cleaned text is empty after processing.\")\n",
    "        return cleaned_text\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(e, 'Cleaning Text Line-Based')\n",
    "        return raw_text\n",
    "\n",
    "# ----------------------- Data Collector Class -----------------------\n",
    "class DataCollector:\n",
    "    def __init__(self, genre_keywords: Dict[str, List[str]] = genre_keywords,\n",
    "                 target_per_genre=10, max_retries=5, backoff_factor=0.3):\n",
    "        self.api_url = 'https://gutendex.com/books/'\n",
    "        self.genre_keywords = genre_keywords\n",
    "        self.target_per_genre = target_per_genre\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        \n",
    "        self.blacklist = set()\n",
    "        self.genre_counts = {genre: 0 for genre in self.genre_keywords}\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        retries = requests.adapters.Retry(\n",
    "            total=self.max_retries,\n",
    "            backoff_factor=self.backoff_factor,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "        adapter = requests.adapters.HTTPAdapter(max_retries=retries)\n",
    "        self.session.mount('http://', adapter)\n",
    "        self.session.mount('https://', adapter)\n",
    "        \n",
    "        self.current_book_id = None\n",
    "\n",
    "    def fetch_books(self, page: int) -> List[Dict]:\n",
    "        try:\n",
    "            response = self.session.get(self.api_url, params={'page': page}, timeout=15)\n",
    "            if response.status_code != 200:\n",
    "                logger.warning(f'Failed to fetch books from page {page}. Status Code: {response.status_code}')\n",
    "                return []\n",
    "            data = response.json()\n",
    "            results = data.get('results', [])\n",
    "            if not results:\n",
    "                logger.info(\"No more books found in API.\")\n",
    "            logger.info(f'Fetched page {page} with {len(results)} books.')\n",
    "            return results\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            logger.error(f'ReadTimeoutError: Timeout while fetching page {page}. Skipping to next page.')\n",
    "            return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            handle_exception(e, f'Fetching Books from Page {page}')\n",
    "            return []\n",
    "\n",
    "    def download_book(self, gutenberg_id: int, retries: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Downloads and returns the plain text of a book. No file saving is done here.\n",
    "        Returns an empty string if download fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            metadata_url = f'https://gutendex.com/books/{gutenberg_id}'\n",
    "            response = self.session.get(metadata_url, timeout=15)\n",
    "            if response.status_code != 200:\n",
    "                logger.warning(f'Failed to fetch metadata for Book {gutenberg_id}. Status Code: {response.status_code}')\n",
    "                return \"\"\n",
    "            book_data = response.json()\n",
    "            formats = book_data.get('formats', {})\n",
    "            \n",
    "            preferred_formats = [\n",
    "                'text/plain; charset=utf-8',\n",
    "                'text/plain; charset=us-ascii',\n",
    "                'text/plain',\n",
    "                'application/octet-stream',\n",
    "                'text/html; charset=utf-8'\n",
    "            ]\n",
    "            \n",
    "            text_url = None\n",
    "            chosen_format = None\n",
    "            for fmt in preferred_formats:\n",
    "                url = formats.get(fmt)\n",
    "                if url:\n",
    "                    text_url = url\n",
    "                    chosen_format = fmt\n",
    "                    break\n",
    "            \n",
    "            if not text_url:\n",
    "                logger.warning(f'No preferred text format found for Book {gutenberg_id}.')\n",
    "                return \"\"\n",
    "            \n",
    "            attempt = 0\n",
    "            while attempt < retries:\n",
    "                try:\n",
    "                    text_response = self.session.get(text_url, timeout=15)\n",
    "                    if text_response.status_code == 200:\n",
    "                        return text_response.text\n",
    "                    else:\n",
    "                        logger.warning(f'Non-200 status code ({text_response.status_code}) for Book {gutenberg_id} from {text_url}.')\n",
    "                        break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    attempt += 1\n",
    "                    logger.warning(f'Attempt {attempt} failed for Book {gutenberg_id} from {text_url}: {e}')\n",
    "                    if attempt >= retries:\n",
    "                        logger.error(f'All {retries} attempts failed for Book {gutenberg_id}.')\n",
    "                        break\n",
    "            \n",
    "            return \"\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            handle_exception(e, f'Download Book {gutenberg_id}')\n",
    "            return \"\"\n",
    "\n",
    "    def get_genre_labels(self, subjects: List[str]) -> List[str]:\n",
    "        try:\n",
    "            mapped_genres = set()\n",
    "            for subject in subjects:\n",
    "                subject_lower = subject.lower()\n",
    "                for genre, keywords in self.genre_keywords.items():\n",
    "                    for keyword in keywords:\n",
    "                        similarity = fuzz.partial_ratio(keyword.lower(), subject_lower)\n",
    "                        if similarity >= 85:\n",
    "                            mapped_genres.add(genre)\n",
    "                            break\n",
    "            return list(mapped_genres)\n",
    "        except Exception as e:\n",
    "            handle_exception(e, 'Mapping Genres')\n",
    "            return []\n",
    "\n",
    "    def collect_data(self) -> List[Dict]:\n",
    "        collected_data = []\n",
    "        current_page = 1\n",
    "        total_genres = len(self.genre_keywords)\n",
    "        target_total = self.target_per_genre * total_genres\n",
    "        logger.info(f\"Starting data collection aiming for {self.target_per_genre} books per genre ({target_total} total).\")\n",
    "        \n",
    "        with tqdm(total=target_total, desc=\"Collecting Books\", unit=\"book\") as pbar:\n",
    "            while sum(self.genre_counts.values()) < target_total:\n",
    "                books = self.fetch_books(current_page)\n",
    "                if not books:\n",
    "                    logger.info(\"No more books to fetch from API.\")\n",
    "                    break\n",
    "                for book in books:\n",
    "                    book_id = book.get('id')\n",
    "                    title = book.get('title', 'Unknown Title')\n",
    "                    languages = book.get('languages', [])\n",
    "                    \n",
    "                    if book_id in self.blacklist:\n",
    "                        continue\n",
    "                    \n",
    "                    self.current_book_id = book_id\n",
    "                    \n",
    "                    if 'en' not in languages:\n",
    "                        continue\n",
    "                    \n",
    "                    subjects = book.get('subjects', [])\n",
    "                    mapped_genres = self.get_genre_labels(subjects)\n",
    "                    \n",
    "                    if not mapped_genres:\n",
    "                        continue\n",
    "                    \n",
    "                    genres_available = [g for g in mapped_genres if self.genre_counts[g] < self.target_per_genre]\n",
    "                    if not genres_available:\n",
    "                        continue\n",
    "                    \n",
    "                    book_text = self.download_book(book_id)\n",
    "                    if not book_text:\n",
    "                        self.blacklist.add(book_id)\n",
    "                        continue\n",
    "                    \n",
    "                    cleaned_text = clean_text_line_based(book_text)\n",
    "                    if not cleaned_text:\n",
    "                        logger.warning(f\"Cleaned text is empty for Book ID: {book_id}. Skipping this book.\")\n",
    "                        self.blacklist.add(book_id)\n",
    "                        continue\n",
    "                    \n",
    "                    collected_data.append({\n",
    "                        \"id\": book_id,\n",
    "                        \"title\": title,\n",
    "                        \"text\": cleaned_text,\n",
    "                        \"genres\": genres_available\n",
    "                    })\n",
    "                    \n",
    "                    for genre in genres_available:\n",
    "                        self.genre_counts[genre] += 1\n",
    "                        pbar.update(1)\n",
    "                        if self.genre_counts[genre] >= self.target_per_genre:\n",
    "                            logger.info(f'Target reached for genre: {genre}')\n",
    "                    \n",
    "                    logger.info(f'Book {book_id} processed with genres: {genres_available}')\n",
    "                \n",
    "                current_page += 1\n",
    "        \n",
    "        log_memory_usage(\"After Data Compilation\")\n",
    "        logger.info(f'Total books successfully processed: {len(collected_data)}')\n",
    "        return collected_data\n",
    "\n",
    "# ----------------------- Data Compilation Execution -----------------------\n",
    "collector = DataCollector(genre_keywords=genre_keywords, target_per_genre=100)\n",
    "collected_data = collector.collect_data()\n",
    "\n",
    "# ----------------------- Data Saving -----------------------\n",
    "output_file = 'collected_books.json'\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(collected_data, f, ensure_ascii=False, indent=4)\n",
    "    logger.info(f\"Successfully saved collected data to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    handle_exception(e, f'Saving Data to {output_file}')\n",
    "\n",
    "# ----------------------- Data Summary -----------------------\n",
    "logger.info(\"=== Data Collection Summary ===\")\n",
    "logger.info(f\"Total Books Processed: {len(collected_data)}\")\n",
    "logger.info(\"Data collection process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
