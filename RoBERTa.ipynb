{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e564b08-5dfb-4053-af80-7d398539bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /users/cs/asalmi/.local/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /users/cs/asalmi/.local/lib/python3.12/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /users/cs/asalmi/.local/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /users/cs/asalmi/.local/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /users/cs/asalmi/.local/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /users/cs/asalmi/.local/lib/python3.12/site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /users/cs/asalmi/.local/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /users/cs/asalmi/.local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /local/pkg/python/root-python-3.12/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85194d7d-4bfc-4c91-b9db-727b61ea82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e33f7ed-ba20-4188-9509-9fa409281a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSON file\n",
    "with open('collected_books.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abf4071-57e5-4b11-bc1d-48c1c97b2de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    _To Mrs. Saville, England._ St. Petersburgh, D...\n",
      "1    MOBY-DICK; or, THE WHALE. By Herman Melville C...\n",
      "2    THE TRAGEDY OF ROMEO AND JULIET by William Sha...\n",
      "3    [Illustration] Mr. Bennet was among the earlie...\n",
      "4    THE SCARLET LETTER. BY NATHANIEL HAWTHORNE. Il...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (df['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf4784e-fd26-488e-b33b-35b1cd2748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the genres\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# Add the encoded labels to the DataFrame\n",
    "df['labels'] = y.tolist()\n",
    "\n",
    "# Get the list of genre classes\n",
    "genre_classes = mlb.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f536b76c-02d5-4d05-87f9-ba8e9bae6fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventure: 100\n",
      "Comedy: 100\n",
      "Drama: 100\n",
      "Fantasy: 100\n",
      "Fiction: 100\n",
      "Horror: 100\n",
      "Mystery: 100\n",
      "Nonfiction: 100\n",
      "Romance: 100\n",
      "Science Fiction: 100\n"
     ]
    }
   ],
   "source": [
    "# Compute genre distribution\n",
    "genre_counts = np.sum(y, axis=0)\n",
    "for genre, count in zip(genre_classes, genre_counts):\n",
    "    print(f\"{genre}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b7981a5-4a21-47ec-9ad9-b82d331d402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts  # Processed texts \n",
    "        self.labels = labels  # Encoded labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Flatten the tensors\n",
    "        item = {key: val.flatten() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eb75a9b-539b-4472-a793-46cae714c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Get the number of labels\n",
    "num_labels = len(genre_classes)\n",
    "\n",
    "# Initialize the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=num_labels,\n",
    "    problem_type='multi_label_classification'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9820a00-a142-47a5-bede-15f59301bde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the text and compute lengths\n",
    "token_lengths = df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# Plot token length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_lengths, bins=50, alpha=0.75)\n",
    "plt.title(\"Distribution of Token Lengths\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8325fa55-8c25-476a-8208-4de8e9c74c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 610\n",
      "Number of validation samples: 152\n"
     ]
    }
   ],
   "source": [
    "# Initialize MultilabelStratifiedShuffleSplit\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract the features and labels\n",
    "X = df['text']\n",
    "y = np.array(df['labels'].tolist())\n",
    "\n",
    "# Perform the split\n",
    "for train_idx, val_idx in msss.split(X, y):\n",
    "    train_texts, val_texts = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    train_labels, val_labels = y[train_idx], y[val_idx]\n",
    "\n",
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of validation samples: {len(val_texts)}\")\n",
    "\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = GenreDataset(train_texts.tolist(), train_labels, tokenizer)\n",
    "val_dataset = GenreDataset(val_texts.tolist(), val_labels, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797aa1d6-c3b5-414c-add3-2601684e3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_optimal_threshold(model, tokenizer, val_texts, val_labels, genre_classes, thresholds=[0.3, 0.4, 0.5, 0.6, 0.7]):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for multi-label classification based on F1 score.\n",
    "\n",
    "    Args:\n",
    "        model (RobertaForSequenceClassification): Trained model.\n",
    "        tokenizer (RobertaTokenizer): Tokenizer.\n",
    "        val_texts (list of str): Validation texts.\n",
    "        val_labels (numpy.ndarray): True binary labels.\n",
    "        genre_classes (list): List of genre class names.\n",
    "        thresholds (list of float, optional): Thresholds to evaluate. Defaults to [0.3, 0.4, 0.5, 0.6, 0.7].\n",
    "\n",
    "    Returns:\n",
    "        float: Optimal threshold.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Preprocess the texts\n",
    "    cleaned_texts = [re.sub(r'[^a-z\\s]', '', text.lower()) for text in val_texts]\n",
    "\n",
    "    # Tokenize the texts\n",
    "    encoding = tokenizer(\n",
    "        cleaned_texts,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Move tensors to device\n",
    "    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probs >= threshold).astype(int)\n",
    "        current_f1 = f1_score(val_labels, y_pred, average='samples', zero_division=0)\n",
    "        print(f\"Threshold: {threshold}, F1 Score: {current_f1:.4f}\")\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"Optimal Threshold: {best_threshold} with F1 Score: {best_f1:.4f}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d1ed236-1e27-45b4-9263-7a20b6c2557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3, F1 Score: 0.2280\n",
      "Threshold: 0.4, F1 Score: 0.2280\n",
      "Threshold: 0.5, F1 Score: 0.1491\n",
      "Threshold: 0.6, F1 Score: 0.0000\n",
      "Threshold: 0.7, F1 Score: 0.0000\n",
      "Optimal Threshold: 0.3 with F1 Score: 0.2280\n",
      "Optimal Threshold determined: 0.3\n"
     ]
    }
   ],
   "source": [
    "optimal_threshold = find_optimal_threshold(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    val_texts=val_texts.tolist(),\n",
    "    val_labels=val_labels,\n",
    "    genre_classes=genre_classes,\n",
    "    thresholds=[0.3, 0.4, 0.5, 0.6, 0.7]\n",
    ")\n",
    "\n",
    "print(f\"Optimal Threshold determined: {optimal_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ef241d-ded5-47cc-8b19-9a313de34d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=6,             # Total number of training epochs\n",
    "    per_device_train_batch_size=8,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # Batch size for evaluation\n",
    "    learning_rate=3.0135064902891538e-05,\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',           # Evaluate every epoch\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Define metrics for evaluation\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Apply sigmoid to logits\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.tensor(preds))\n",
    "\n",
    "    # Threshold predictions\n",
    "    y_pred = (probs >= 0.3).int().numpy()\n",
    "\n",
    "    # Debug: Inspect some predictions and labels\n",
    "    if not hasattr(compute_metrics, \"has_printed\"):\n",
    "        print(\"Sample Predictions and Labels:\")\n",
    "        for i in range(min(5, len(y_pred))):\n",
    "            print(f\"Prediction: {y_pred[i]}\")\n",
    "            print(f\"Label: {labels[i]}\")\n",
    "            print(\"---\")\n",
    "        compute_metrics.has_printed = True\n",
    "\n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, y_pred, average='samples', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, y_pred)\n",
    "    f1_macro = f1_score(labels, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'f1_macro': f1_macro,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3ba4919-e4e9-4f35-af63-47d019cc9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36188dde-22eb-4a56-b8e4-40a5730835ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 11:55:12,540] A new study created in memory with name: no-name-808b2b14-462d-4e4b-8b7c-294772a6f9dd\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 53:34, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.649992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228044</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.397534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.388471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.383550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.375737</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.361986</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.086623</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.079825</td>\n",
       "      <td>0.124987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions and Labels:\n",
      "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
      "Label: [1. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      "---\n",
      "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
      "Label: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "---\n",
      "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
      "Label: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "---\n",
      "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
      "Label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "---\n",
      "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
      "Label: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 12:50:53,420] Trial 0 finished with value: 0.2280438859386228 and parameters: {'learning_rate': 4.6973715136163516e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 0.2280438859386228.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 1:07:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228044</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.622647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228044</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558800</td>\n",
       "      <td>0.523825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131067</td>\n",
       "      <td>0.993421</td>\n",
       "      <td>0.227087</td>\n",
       "      <td>0.231788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.434419</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.029605</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.401420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.391620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.388082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 14:01:05,265] Trial 1 finished with value: 0.2280438859386228 and parameters: {'learning_rate': 1.7199248918616902e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.2280438859386228.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 59:55, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.631700</td>\n",
       "      <td>0.627886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228044</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>0.504391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130702</td>\n",
       "      <td>0.950658</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.230182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.411812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.390685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.404400</td>\n",
       "      <td>0.386976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.384993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.380801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 15:03:21,419] Trial 2 finished with value: 0.2280438859386228 and parameters: {'learning_rate': 4.278068640965927e-05, 'num_train_epochs': 7, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.2280438859386228.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 51:12, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.591238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228044</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.459763</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.133516</td>\n",
       "      <td>0.257675</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>0.106715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.399935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.388482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.386178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>0.383342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 15:56:52,614] Trial 3 finished with value: 0.2280438859386228 and parameters: {'learning_rate': 4.9735843734138455e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.2280438859386228.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 35:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.576700</td>\n",
       "      <td>0.547342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228044</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.452497</td>\n",
       "      <td>0.046053</td>\n",
       "      <td>0.131689</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.139793</td>\n",
       "      <td>0.083628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.402288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.390429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:34:20,994] Trial 4 finished with value: 0.2280438859386228 and parameters: {'learning_rate': 1.0854070278305307e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 0.2280438859386228.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='462' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [462/462 53:43, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.402464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.387057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.382056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.365982</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.067982</td>\n",
       "      <td>0.067434</td>\n",
       "      <td>0.064693</td>\n",
       "      <td>0.112753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.343578</td>\n",
       "      <td>0.151316</td>\n",
       "      <td>0.235746</td>\n",
       "      <td>0.224781</td>\n",
       "      <td>0.222588</td>\n",
       "      <td>0.259129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.328826</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.362939</td>\n",
       "      <td>0.351316</td>\n",
       "      <td>0.364714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 17:29:58,009] Trial 5 finished with value: 0.35131578947368414 and parameters: {'learning_rate': 3.0135064902891538e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 8}. Best is trial 5 with value: 0.35131578947368414.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 1:08:50, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.324456</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.353070</td>\n",
       "      <td>0.345614</td>\n",
       "      <td>0.359199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.320317</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.406798</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.394956</td>\n",
       "      <td>0.422410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.316118</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.427632</td>\n",
       "      <td>0.416228</td>\n",
       "      <td>0.448016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.312909</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.419956</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.417105</td>\n",
       "      <td>0.467615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.314552</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.410088</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>0.403947</td>\n",
       "      <td>0.447632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>0.309355</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.433662</td>\n",
       "      <td>0.405482</td>\n",
       "      <td>0.456456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.314568</td>\n",
       "      <td>0.217105</td>\n",
       "      <td>0.384868</td>\n",
       "      <td>0.434759</td>\n",
       "      <td>0.390570</td>\n",
       "      <td>0.436882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.311235</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.426535</td>\n",
       "      <td>0.438048</td>\n",
       "      <td>0.419518</td>\n",
       "      <td>0.451103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 18:41:10,832] Trial 6 finished with value: 0.41951754385964907 and parameters: {'learning_rate': 4.2719959149685716e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.41951754385964907.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [308/308 36:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.311760</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.419518</td>\n",
       "      <td>0.449607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.320433</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.408991</td>\n",
       "      <td>0.450110</td>\n",
       "      <td>0.412719</td>\n",
       "      <td>0.442375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.320472</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.474232</td>\n",
       "      <td>0.439254</td>\n",
       "      <td>0.460249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.331899</td>\n",
       "      <td>0.243421</td>\n",
       "      <td>0.390351</td>\n",
       "      <td>0.430373</td>\n",
       "      <td>0.392544</td>\n",
       "      <td>0.424797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 19:19:30,813] Trial 7 finished with value: 0.4392543859649123 and parameters: {'learning_rate': 2.1493496903820993e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 8}. Best is trial 7 with value: 0.4392543859649123.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='539' max='539' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [539/539 1:03:26, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.320155</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.434759</td>\n",
       "      <td>0.410746</td>\n",
       "      <td>0.435661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.341818</td>\n",
       "      <td>0.230263</td>\n",
       "      <td>0.378289</td>\n",
       "      <td>0.413925</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.417739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.341389</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.432018</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.420395</td>\n",
       "      <td>0.444114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.348265</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.457785</td>\n",
       "      <td>0.433991</td>\n",
       "      <td>0.449649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.357575</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>0.415570</td>\n",
       "      <td>0.411184</td>\n",
       "      <td>0.396053</td>\n",
       "      <td>0.436544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.389184</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.434430</td>\n",
       "      <td>0.443131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.378149</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.483004</td>\n",
       "      <td>0.448246</td>\n",
       "      <td>0.462389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 20:24:46,938] Trial 8 finished with value: 0.44824561403508767 and parameters: {'learning_rate': 3.1767461460161386e-05, 'num_train_epochs': 7, 'per_device_train_batch_size': 8}. Best is trial 8 with value: 0.44824561403508767.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='539' max='539' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [539/539 1:03:19, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.375139</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.472588</td>\n",
       "      <td>0.481360</td>\n",
       "      <td>0.458553</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.375773</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.459430</td>\n",
       "      <td>0.448246</td>\n",
       "      <td>0.472458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.377298</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.421272</td>\n",
       "      <td>0.456395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.382910</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.452851</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.434430</td>\n",
       "      <td>0.456303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.390296</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.427632</td>\n",
       "      <td>0.430373</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>0.444222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.386578</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.446272</td>\n",
       "      <td>0.492873</td>\n",
       "      <td>0.446053</td>\n",
       "      <td>0.482159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.398295</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.445175</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.423026</td>\n",
       "      <td>0.456712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 21:30:00,910] Trial 9 finished with value: 0.45855263157894727 and parameters: {'learning_rate': 1.0172355231077042e-05, 'num_train_epochs': 7, 'per_device_train_batch_size': 8}. Best is trial 9 with value: 0.45855263157894727.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 1:29:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.374527</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.458772</td>\n",
       "      <td>0.474258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.376853</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.453289</td>\n",
       "      <td>0.476654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.379358</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.431140</td>\n",
       "      <td>0.459465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.383355</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.435526</td>\n",
       "      <td>0.459670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.392050</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.433662</td>\n",
       "      <td>0.421491</td>\n",
       "      <td>0.451920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.388337</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.441886</td>\n",
       "      <td>0.492873</td>\n",
       "      <td>0.442763</td>\n",
       "      <td>0.485672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.406675</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.461623</td>\n",
       "      <td>0.466557</td>\n",
       "      <td>0.448026</td>\n",
       "      <td>0.466799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.406849</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.451206</td>\n",
       "      <td>0.432018</td>\n",
       "      <td>0.459870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.409730</td>\n",
       "      <td>0.335526</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.454496</td>\n",
       "      <td>0.447807</td>\n",
       "      <td>0.478524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.405685</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.464125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 23:00:59,995] Trial 10 finished with value: 0.45877192982456133 and parameters: {'learning_rate': 1.0705675604320086e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 1:29:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.374641</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.471483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.378166</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.452851</td>\n",
       "      <td>0.474922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.379849</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.424781</td>\n",
       "      <td>0.452417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.383881</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.438816</td>\n",
       "      <td>0.462687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.390575</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.430373</td>\n",
       "      <td>0.416009</td>\n",
       "      <td>0.446886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.502741</td>\n",
       "      <td>0.449029</td>\n",
       "      <td>0.489633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.404897</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.471491</td>\n",
       "      <td>0.473136</td>\n",
       "      <td>0.455921</td>\n",
       "      <td>0.472498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.408006</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.432018</td>\n",
       "      <td>0.434759</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.443454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.410348</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.441338</td>\n",
       "      <td>0.434649</td>\n",
       "      <td>0.471388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.406226</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.434759</td>\n",
       "      <td>0.423246</td>\n",
       "      <td>0.456298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 00:32:03,812] Trial 11 finished with value: 0.45592105263157895 and parameters: {'learning_rate': 1.0141308578295247e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 1:29:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.397924</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.452851</td>\n",
       "      <td>0.435746</td>\n",
       "      <td>0.471344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.402153</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.459430</td>\n",
       "      <td>0.454496</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.408338</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.461910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.429244</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.423246</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.454097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.421373</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.461623</td>\n",
       "      <td>0.447494</td>\n",
       "      <td>0.475302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.448814</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.476425</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>0.459095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.446399</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.474232</td>\n",
       "      <td>0.453728</td>\n",
       "      <td>0.467903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.460029</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.415022</td>\n",
       "      <td>0.394956</td>\n",
       "      <td>0.437048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.456675</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.446272</td>\n",
       "      <td>0.452303</td>\n",
       "      <td>0.435088</td>\n",
       "      <td>0.458427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.460492</td>\n",
       "      <td>0.335526</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.465461</td>\n",
       "      <td>0.450439</td>\n",
       "      <td>0.476880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 02:03:33,992] Trial 12 finished with value: 0.4537280701754386 and parameters: {'learning_rate': 1.4989469483690678e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='693' max='693' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [693/693 1:20:32, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.444746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.449035</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.456689</td>\n",
       "      <td>0.441009</td>\n",
       "      <td>0.457980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.453537</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.441573</td>\n",
       "      <td>0.470967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.461291</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.428728</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.423904</td>\n",
       "      <td>0.445094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.478319</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.445175</td>\n",
       "      <td>0.424123</td>\n",
       "      <td>0.444859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.498865</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.427632</td>\n",
       "      <td>0.436952</td>\n",
       "      <td>0.417763</td>\n",
       "      <td>0.430083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.501235</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.411184</td>\n",
       "      <td>0.424890</td>\n",
       "      <td>0.402412</td>\n",
       "      <td>0.421772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.491276</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.421601</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.429824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.489373</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>0.421601</td>\n",
       "      <td>0.408991</td>\n",
       "      <td>0.440549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 03:26:00,823] Trial 13 finished with value: 0.44157268170426067 and parameters: {'learning_rate': 1.3306074922486437e-05, 'num_train_epochs': 9, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='693' max='693' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [693/693 1:18:59, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.450110</td>\n",
       "      <td>0.435746</td>\n",
       "      <td>0.464614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.455895</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.462171</td>\n",
       "      <td>0.440570</td>\n",
       "      <td>0.459445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.461484</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.445175</td>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.466115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.474431</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.480811</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.450750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.495511</td>\n",
       "      <td>0.243421</td>\n",
       "      <td>0.411184</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.407675</td>\n",
       "      <td>0.428673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.528021</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.384868</td>\n",
       "      <td>0.411732</td>\n",
       "      <td>0.380482</td>\n",
       "      <td>0.396363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.510047</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.408114</td>\n",
       "      <td>0.427505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.498074</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>0.413377</td>\n",
       "      <td>0.423246</td>\n",
       "      <td>0.402851</td>\n",
       "      <td>0.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.494590</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.451206</td>\n",
       "      <td>0.423026</td>\n",
       "      <td>0.456702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 04:46:54,201] Trial 14 finished with value: 0.45504385964912286 and parameters: {'learning_rate': 1.2558038703190195e-05, 'num_train_epochs': 9, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 25:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.467525</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.466009</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.452412</td>\n",
       "      <td>0.459935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.468530</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.439145</td>\n",
       "      <td>0.422713</td>\n",
       "      <td>0.446871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.475396</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.438048</td>\n",
       "      <td>0.423590</td>\n",
       "      <td>0.449532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 05:14:53,121] Trial 15 finished with value: 0.4524122807017544 and parameters: {'learning_rate': 1.82858351155806e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='385' max='385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [385/385 44:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.467995</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.446272</td>\n",
       "      <td>0.449013</td>\n",
       "      <td>0.431485</td>\n",
       "      <td>0.452382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.471544</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.432018</td>\n",
       "      <td>0.425987</td>\n",
       "      <td>0.416573</td>\n",
       "      <td>0.448130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.475686</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.405702</td>\n",
       "      <td>0.408443</td>\n",
       "      <td>0.393202</td>\n",
       "      <td>0.431506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.481035</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.453399</td>\n",
       "      <td>0.432581</td>\n",
       "      <td>0.467154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.501222</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.428728</td>\n",
       "      <td>0.428728</td>\n",
       "      <td>0.415351</td>\n",
       "      <td>0.438276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 06:01:41,617] Trial 16 finished with value: 0.4325814536340852 and parameters: {'learning_rate': 1.1789546806326372e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='616' max='616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [616/616 1:12:04, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.478822</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.446820</td>\n",
       "      <td>0.431046</td>\n",
       "      <td>0.459489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.491650</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.432018</td>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.424123</td>\n",
       "      <td>0.447821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.490865</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.412829</td>\n",
       "      <td>0.405921</td>\n",
       "      <td>0.438439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.504719</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.396382</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.420988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.525199</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.400219</td>\n",
       "      <td>0.408443</td>\n",
       "      <td>0.387061</td>\n",
       "      <td>0.412552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.549686</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421601</td>\n",
       "      <td>0.408772</td>\n",
       "      <td>0.423568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.525285</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.406798</td>\n",
       "      <td>0.436680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.532316</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.450110</td>\n",
       "      <td>0.425219</td>\n",
       "      <td>0.447167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 07:15:43,772] Trial 17 finished with value: 0.431046365914787 and parameters: {'learning_rate': 1.4996147531159034e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='693' max='693' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [693/693 1:21:01, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.444627</td>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.462163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.504189</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.410088</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.393640</td>\n",
       "      <td>0.424088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.506854</td>\n",
       "      <td>0.309211</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.399342</td>\n",
       "      <td>0.404831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.578665</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.410307</td>\n",
       "      <td>0.434306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.542780</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.434868</td>\n",
       "      <td>0.441111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.556854</td>\n",
       "      <td>0.243421</td>\n",
       "      <td>0.419956</td>\n",
       "      <td>0.442434</td>\n",
       "      <td>0.410746</td>\n",
       "      <td>0.428566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.547228</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.417763</td>\n",
       "      <td>0.434759</td>\n",
       "      <td>0.407018</td>\n",
       "      <td>0.431426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.435307</td>\n",
       "      <td>0.413816</td>\n",
       "      <td>0.428844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.567302</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.451206</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.452990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 08:38:40,089] Trial 18 finished with value: 0.4348684210526315 and parameters: {'learning_rate': 2.3775923400561827e-05, 'num_train_epochs': 9, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 0.45877192982456133.\n",
      "/tmp/ipykernel_579486/3482760143.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 1:25:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.533239</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.444737</td>\n",
       "      <td>0.458913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.524522</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.470943</td>\n",
       "      <td>0.442325</td>\n",
       "      <td>0.454683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.525022</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.461623</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.452013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.529151</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.433114</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>0.444809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.522955</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.425658</td>\n",
       "      <td>0.437302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.528210</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.422368</td>\n",
       "      <td>0.441705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.527939</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.427193</td>\n",
       "      <td>0.445312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.530159</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.422149</td>\n",
       "      <td>0.411404</td>\n",
       "      <td>0.433915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.530320</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.415789</td>\n",
       "      <td>0.427044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.528129</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.436952</td>\n",
       "      <td>0.422368</td>\n",
       "      <td>0.451560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 10:05:52,081] Trial 19 finished with value: 0.4447368421052631 and parameters: {'learning_rate': 1.3981867629931754e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 16}. Best is trial 10 with value: 0.45877192982456133.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 0.45877192982456133\n",
      "Best hyperparameters: \n",
      "  learning_rate: 1.0705675604320086e-05\n",
      "  num_train_epochs: 10\n",
      "  per_device_train_batch_size: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 10)\n",
    "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16, 32])\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=num_train_epochs,             # Total number of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # Batch size for evaluation\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',           # Evaluate every epoch\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    return eval_result['eval_f1']\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(\"Best hyperparameters: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e86c4c9-110f-42a7-8735-66c593b2a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='462' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [462/462 52:00, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.320677</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.393640</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.391228</td>\n",
       "      <td>0.434838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.318470</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.393640</td>\n",
       "      <td>0.398026</td>\n",
       "      <td>0.383772</td>\n",
       "      <td>0.425255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.327534</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.392544</td>\n",
       "      <td>0.439145</td>\n",
       "      <td>0.399781</td>\n",
       "      <td>0.428301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.324258</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.412829</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.412719</td>\n",
       "      <td>0.437762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.344598</td>\n",
       "      <td>0.230263</td>\n",
       "      <td>0.389254</td>\n",
       "      <td>0.410088</td>\n",
       "      <td>0.382801</td>\n",
       "      <td>0.417680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.358036</td>\n",
       "      <td>0.243421</td>\n",
       "      <td>0.383772</td>\n",
       "      <td>0.400219</td>\n",
       "      <td>0.378289</td>\n",
       "      <td>0.404134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=462, training_loss=0.18040739702971983, metrics={'train_runtime': 3128.5891, 'train_samples_per_second': 1.17, 'train_steps_per_second': 0.148, 'total_flos': 240763908188160.0, 'train_loss': 0.18040739702971983, 'epoch': 6.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfe36c9a-4b0e-473a-ba14-51d49be7db24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "# Create a bar chart for metrics\n",
    "metrics = ['Precision', 'Recall', 'F1 Score', 'F1 Macro']\n",
    "values = [eval_result['eval_precision'], eval_result['eval_recall'], eval_result['eval_f1'], eval_result['eval_f1_macro']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics, values, alpha=0.75)\n",
    "plt.title(\"Evaluation Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)  # Scores are between 0 and 1\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeabff-a0da-4de9-96da-61d8ecc9d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained('./model')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab849701-5b47-4d5d-b736-b12564d64b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted genres: ['Adventure: 0.008485725', 'Comedy: 0.0005976711', 'Drama: 0.00033271176', 'Fantasy: 0.038390428', 'Fiction: 0.0075266073', 'Horror: 0.791661', 'Mystery: 0.0033068573', 'Nonfiction: 0.0002585595', 'Romance: 0.0007731407', 'Science Fiction: 0.0047708983']\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('./model')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./tokenizer')\n",
    "\n",
    "# Assuming genre_classes is defined elsewhere, for example:\n",
    "# genre_classes = [\"Mystery\", \"Fantasy\", \"Horror\", \"Sci-Fi\", ...] \n",
    "\n",
    "# The predict_genres function (as you defined it)\n",
    "def predict_genres(text, model, tokenizer, genre_classes, threshold=0.5):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        raise ValueError(\"Input text must be a non-empty string.\")\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = probs.cpu().numpy()[0]\n",
    "\n",
    "    predicted_genres = [genre_classes[i] + \": \" + str(prob) for i, prob in enumerate(probs) if prob >= threshold]\n",
    "    return predicted_genres\n",
    "\n",
    "story = \"\"\"\n",
    "The Whispering Shadows \n",
    "\n",
    "Deep within the ancient woods of Eldenreach, where sunlight struggled to pierce the dense canopy, stood the village of Greystone. Isolated and steeped in superstition, the villagers spoke in hushed tones of the \"Whispering Shadows,\" a legend about spirits who lured the unwary into the forest, never to return.\n",
    "\n",
    "Aven Callis, a young healer with a mysterious past, had never believed the tales. That was until the night her younger brother, Finn, disappeared into the woods without a trace. Desperate to find him, Aven armed herself with little more than a lantern and a dagger, venturing into the forbidden forest under the pale light of the crescent moon.\n",
    "\n",
    "The woods were alive with strange noisesbranches creaking like whispered warnings and the rustle of unseen movements. The deeper she ventured, the colder the air became, and the shadows seemed to stretch and twist unnaturally. \n",
    "\n",
    "\"Aven...\" a voice called, soft and mournful. She froze, her heart pounding. It was Finns voice, but it was wrongtoo distant, too hollow.\n",
    "\n",
    "\"Finn?\" she called, her voice trembling. She followed the sound, only to find herself at the edge of a glade, where the air shimmered as if the world itself was holding its breath. In the center stood a towering stone monolith, etched with glowing runes that pulsed like a heartbeat.\n",
    "\n",
    "As Aven approached, the runes flared, and a figure stepped from the shadows. He was tall, cloaked in black, with eyes that gleamed like molten gold. \"You seek the boy,\" he said, his voice smooth as silk. \"But do you truly know what he is?\"\n",
    "\n",
    "\"What are you talking about?\" Aven demanded, clutching her dagger tightly.\n",
    "\n",
    "The man smiled faintly, his gaze piercing. \"The Whispering Shadows dont take without reason. The boy... he belongs to them now. But there is a way to reclaim him.\"\n",
    "\n",
    "Avens grip on the dagger tightened. \"Tell me how.\"\n",
    "\n",
    "\"You must walk the Shadow Path,\" he said, gesturing toward the monolith. \"But bewareevery step will strip away a part of you. Your past, your secrets, even your soul. Do you have the strength to face the truth?\"\n",
    "\n",
    "Without hesitation, Aven stepped toward the monolith. The moment her hand touched the cold stone, the world around her dissolved into darkness. She found herself on a narrow path lit by ghostly flames, each step revealing fragments of her forgotten memories: a mothers desperate cries, a brother born under a blood moon, and her own whispered promise to protect him at any cost.\n",
    "\n",
    "The path led to a massive shadowy figure, its form shifting and writhing. Finn stood before it, his eyes black as obsidian, his voice a hollow echo. \"You were supposed to keep me safe.\"\n",
    "\n",
    "\"I will,\" Aven said, tears streaming down her face. She plunged her dagger into the heart of the shadow, a blinding light erupting around her.\n",
    "\n",
    "When the light faded, Aven was back in the glade, Finn unconscious in her arms. The runes on the monolith had gone dark, and the golden-eyed man was nowhere to be seen.\n",
    "\n",
    "But as she carried Finn back to Greystone, she felt the weight of the shadows in her mind. They had whispered secrets she could never forget, and Aven knew the price of her choice was far from fully paid.\n",
    "\"\"\"\n",
    "\n",
    "# Predict genres\n",
    "predicted_genres = predict_genres(\n",
    "    text=story,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    genre_classes=genre_classes,\n",
    "    threshold= 0.3\n",
    ")\n",
    "\n",
    "print(\"Predicted genres:\", predicted_genres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
